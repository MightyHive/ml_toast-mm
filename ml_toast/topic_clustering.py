# Copyright 2024 Google LLC.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Topic clustering module.

This module provides functionality to cluster embeddings generated by Google's
Universal Sentence Encoder Multilingual (USE-M) model using K-Means and HDBSCAN
(after dimensionality reduction with UMAP). K-Means clustering provides a
high-level categorization of the general themes within the corpus (i.e. global
structure), whereas UMAP + HDBSCAN dive deeper to identify more fine-grained
groups (i.e. local structure). Once clusters have been identified, two
approaches are used to generate semantically relevant labels for them:

1.  K-Means: Count the most repeated words in the corpus (after stop words
    removal*), generate embeddings with USE-M, and then select the embedding
    with the highest cosine similarity to each cluster's center.
2.  HDBSCAN: Fit a TF-IDF vectorizer to the corpus and select the top three
    unigrams / bigrams per cluster with high TF-IDF values.

*Lemmatization and/or stemming are also considered best practices for NLP tasks.
Though English language lemmatization is quite straightforward (and we would
highly recommend its use should the underlying corpus be primarily in English),
multilingual lemmatization is an active research topic with several caveats due
to the morphological irregularities and substantial word inflections in
different languages. As a result, this module does not cover these concepts.
"""

import logging
import os
import pathlib
from typing import Mapping, Optional, Sequence, Tuple

from absl import app
from absl import flags

import hyperopt
import pandas as pd
import tensorflow.compat.v2 as tf
import tensorflow_hub as hub

from ml_toast import topic_clusterer_hdbscan
from ml_toast import topic_clusterer_kmeans

# Initialization constants
_INPUT_DATA_PATH = 'samples/data.csv'
_OUTPUT_DATA_PATH = 'output/data.csv'
_INPUT_DATA_ID = 'data'
_INPUT_DATA_COLUMN = 'document'
_OUTPUT_KMEANS_COLUMN = 'topics_kmeans'
_OUTPUT_HDBSCAN_COLUMN = 'topics_hdbscan'
_STOP_WORDS = None
_KMEANS_CLUSTERS = range(5, 16)
_CLUSTERING_PARAMS = {
    'umap_n_neighbors': 15,
    'umap_n_components': 30,
    'umap_random_state': None,
    'hdbscan_min_cluster_size': 20,
    'hdbscan_min_samples': 5,
    'threshold_unclustered': 0.4,
    'threshold_recluster': 0.8,
}
_HYPERPARAMETER_TUNING = True

_FLAG_INPUT_DATA_PATH = flags.DEFINE_string(
    'csv_data_path', _INPUT_DATA_PATH,
    'Path to a csv file containing the data to use. Defaults to the path '
    f'{_INPUT_DATA_PATH} relative to the current (root) directory.')
_FLAG_OUTPUT_DATA_PATH = flags.DEFINE_string(
    'output_path', _OUTPUT_DATA_PATH,
    'Path where the output should be stored. Defaults to the path '
    f'{_OUTPUT_DATA_PATH} relative to the current (root) directory.')
_FLAG_INPUT_DATA_ID = flags.DEFINE_string(
    'data_id', _INPUT_DATA_ID,
    'Identifier of the input data. Useful for logging when running the module '
    f'with different inputs. Defaults to the value "{_INPUT_DATA_ID}"')
_FLAG_INPUT_DATA_COLUMN = flags.DEFINE_string(
    'input_col', _INPUT_DATA_COLUMN,
    'The column in the input data corresponding to the documents to cluster. '
    f'Defaults to the value "{_INPUT_DATA_COLUMN}"')
_FLAG_OUTPUT_KMEANS_COLUMN = flags.DEFINE_string(
    'output_kmeans_col', _OUTPUT_KMEANS_COLUMN,
    'The column to write the generated K-Means cluster assignments to. '
    f'Defaults to the value "{_OUTPUT_KMEANS_COLUMN}"')
_FLAG_OUTPUT_HDBSCAN_COLUMN = flags.DEFINE_string(
    'output_hdbscan_col', _OUTPUT_HDBSCAN_COLUMN,
    'The column to write the generated HDBSCAN cluster assignments to. '
    f'Defaults to the value "{_OUTPUT_HDBSCAN_COLUMN}"')
_FLAG_STOP_WORDS = flags.DEFINE_list(
    'stop_words', _STOP_WORDS, 'List of custom words to use as stop words. '
    'Defaults to None')
_FLAG_KMEANS_CLUSTERS = flags.DEFINE_multi_integer(
    'kmeans_clusters', _KMEANS_CLUSTERS,
    'List of clusters to use to identify the optimal value of K. '
    'Defaults to the range 5-15')
_FLAG_UMAP_N_NEIGHBORS = flags.DEFINE_integer(
    'umap_n_neighbors', _CLUSTERING_PARAMS['umap_n_neighbors'],
    'n_neighbors hyperparameter for UMAP. Defaults to '
    f"{_CLUSTERING_PARAMS['umap_n_neighbors']}")
_FLAG_UMAP_N_COMPONENTS = flags.DEFINE_integer(
    'umap_n_components', _CLUSTERING_PARAMS['umap_n_components'],
    'n_components hyperparameter for UMAP. Defaults to '
    f"{_CLUSTERING_PARAMS['umap_n_components']}")
_FLAG_UMAP_RANDOM_STATE = flags.DEFINE_integer(
    'umap_random_state', _CLUSTERING_PARAMS['umap_random_state'],
    'random seed for UMAP. Defaults to '
    f"{_CLUSTERING_PARAMS['umap_random_state']}. Consider setting an explicit "
    'value (e.g. 32) for reproducability')
_FLAG_HDBSCAN_MIN_CLUSTER_SIZE = flags.DEFINE_integer(
    'hdbscan_min_cluster_size', _CLUSTERING_PARAMS['hdbscan_min_cluster_size'],
    'min_cluster_size hyperparameter for HDBSCAN. Defaults to '
    f"{_CLUSTERING_PARAMS['hdbscan_min_cluster_size']}")
_FLAG_HDBSCAN_MIN_SAMPLES = flags.DEFINE_integer(
    'hdbscan_min_samples', _CLUSTERING_PARAMS['hdbscan_min_samples'],
    'min_samples hyperparameter for HDBSCAN. Defaults to '
    f"{_CLUSTERING_PARAMS['hdbscan_min_samples']}")
_FLAG_OPT_THRESHOLD_UNCLUSTERED = flags.DEFINE_float(
    'opt_threshold_unclustered', _CLUSTERING_PARAMS['threshold_unclustered'],
    'Threshold for clustering unassigned data points. Defaults to '
    f"{_CLUSTERING_PARAMS['threshold_unclustered']}")
_FLAG_OPT_THRESHOLD_RECLUSTER = flags.DEFINE_float(
    'opt_threshold_recluster', _CLUSTERING_PARAMS['threshold_recluster'],
    'Threshold for reclustering for more consistent cluster interpretations. '
    f"Defaults to {_CLUSTERING_PARAMS['threshold_recluster']}")
_FLAG_HYPERPARAMETER_TUNING = flags.DEFINE_boolean(
    'hyperparameter_tuning', _HYPERPARAMETER_TUNING,
    'Whether to tune hyperparameters for UMAP + HDBSCAN. Defaults to True')

# Reduce verbosity of tensorflow
tf.get_logger().setLevel(logging.ERROR)

# Reduce verbosity of hyperopt
for logger in [
    'hyperopt.tpe',
    'hyperopt.fmin',
    'hyperopt.pyll.base',
]:
  logging.getLogger(logger).setLevel(logging.ERROR)


class TopicClustering(object):
  """Handles the clustering of documents into semantically-relevant topics.

  Attributes:
    data_id: Identifier of the data to be clustered. Useful for logging when
      running the module with different inputs.
    input_col: The column in the input data corresponding to the documents to
      cluster. Defaults to the value 'document'.
    stop_words: List of custom words to use as stop words.
    kmeans_clusters: List of clusters to use to identify the optimal value of K.
    hdbscan_clustering_params: Default hyperparameters for UMAP + HDBSCAN, and
      thresholds for optimizing the generated clusters.
    do_hdbscan_hyperopt: Whether to tune hyperparameters for UMAP + HDBSCAN.
      Defaults to True.
    hdbscan_hyperopt_params: Hyperopt pyll graphs for tuning UMAP and HDBSCAN
      hyperparameters.
    model: Model object loaded from TensorFlow Hub.
  """

  def __init__(self,
               data_id: str = _INPUT_DATA_ID,
               input_col: str = _INPUT_DATA_COLUMN,
               stop_words: Optional[Sequence[str]] = None,
               kmeans_clusters: Sequence[int] = _KMEANS_CLUSTERS,
               hdbscan_clustering_params: Optional[Mapping[str, int]] = None,
               do_hdbscan_hyperopt=_HYPERPARAMETER_TUNING) -> None:
    """Initializer."""
    self.data_id = data_id
    self.input_col = input_col
    self.stop_words = stop_words or _STOP_WORDS
    self.kmeans_clusters = kmeans_clusters
    self.hdbscan_clustering_params = (
        hdbscan_clustering_params or _CLUSTERING_PARAMS)

    self.hdbscan_hyperopt_params = {
        'umap_n_neighbors':
            hyperopt.hp.choice('umap_n_neighbors', range(5, 31)),
        'umap_n_components':
            hyperopt.hp.choice('umap_n_components', range(5, 31)),
        'umap_random_state':
            self.hdbscan_clustering_params.get('umap_random_state', None),
        'hdbscan_min_cluster_size':
            hyperopt.hp.choice('hdbscan_min_cluster_size', range(5, 31)),
        'hdbscan_min_samples':
            hyperopt.hp.choice('hdbscan_min_samples', range(5, 31)),
    } if do_hdbscan_hyperopt else None
    self.model = hub.load(
        'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3')

  def determine_topics(
      self, data: pd.DataFrame) -> Tuple[Sequence[str], Sequence[str]]:
    """Determines the topic for a given dataframe containing documents.

    Args:
      data: the full set of data to classify.

    Returns:
      A tuple of generated K-Means and HDBSCAN topics.
    """
    documents = data[self.input_col]

    kmeans_instance = topic_clusterer_kmeans.TopicClustererKmeans(
        self.data_id, self.model, self.stop_words, self.kmeans_clusters)
    topics_kmeans = kmeans_instance.modelling_pipeline(pd.DataFrame(documents))

    hdbscan_instance = topic_clusterer_hdbscan.TopicClustererHdbscan(
        self.data_id, self.model, self.stop_words,
        self.hdbscan_clustering_params, self.hdbscan_hyperopt_params)
    topics_hdbscan = hdbscan_instance.modelling_pipeline(
        pd.DataFrame(documents))

    return (topics_kmeans.to_list(),
            topics_hdbscan.fillna('[]').apply(str).to_list())


def main(argv: Sequence[str]) -> None:
  del argv  # Unused

  logging.info('BEGIN - TopicClustering')

  clustering_params = {
      'umap_n_neighbors': _FLAG_UMAP_N_NEIGHBORS.value,
      'umap_n_components': _FLAG_UMAP_N_COMPONENTS.value,
      'umap_random_state': _FLAG_UMAP_RANDOM_STATE.value,
      'hdbscan_min_cluster_size': _FLAG_HDBSCAN_MIN_CLUSTER_SIZE.value,
      'hdbscan_min_samples': _FLAG_HDBSCAN_MIN_SAMPLES.value,
      'threshold_unclustered': _FLAG_OPT_THRESHOLD_UNCLUSTERED.value,
      'threshold_recluster': _FLAG_OPT_THRESHOLD_RECLUSTER.value,
  }

  input_path = os.path.abspath(
      os.path.join(
          os.path.dirname(__file__), os.pardir, _FLAG_INPUT_DATA_PATH.value))
  if not pathlib.Path.exists(pathlib.Path(input_path)):
    raise ValueError(f'Could not find data at input path: "{input_path}"!')

  input_data = pd.read_csv(input_path)
  if (
      input_data.empty
      or _FLAG_INPUT_DATA_COLUMN.value not in input_data.columns):
    raise ValueError(
        f'Empty or missing column "{_FLAG_INPUT_DATA_COLUMN.value}" in the '
        f'input data at path "{_FLAG_INPUT_DATA_PATH.value}"!')

  instance = TopicClustering(
      data_id=_FLAG_INPUT_DATA_ID.value,
      input_col=_FLAG_INPUT_DATA_COLUMN.value,
      stop_words=_FLAG_STOP_WORDS.value,
      kmeans_clusters=_FLAG_KMEANS_CLUSTERS.value,
      hdbscan_clustering_params=clustering_params,
      do_hdbscan_hyperopt=_FLAG_HYPERPARAMETER_TUNING.value)

  topics_kmeans, topics_hbdscan = instance.determine_topics(data=input_data)

  input_data[_FLAG_OUTPUT_KMEANS_COLUMN.value] = topics_kmeans
  input_data[_FLAG_OUTPUT_HDBSCAN_COLUMN.value] = topics_hbdscan
  input_data.to_csv(
      os.path.abspath(
          os.path.join(
              os.path.dirname(__file__),
              os.pardir,
              _FLAG_OUTPUT_DATA_PATH.value)),
      index=False)

  logging.info('END - TopicClustering')


if __name__ == '__main__':
  flags.mark_flag_as_required(_FLAG_INPUT_DATA_PATH.name)
  flags.mark_flag_as_required(_FLAG_OUTPUT_DATA_PATH.name)
  app.run(main)
